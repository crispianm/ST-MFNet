{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from data import testsets\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from data.datasets import *\n",
    "from trainers.distiller import Distiller\n",
    "from torch.utils.data import DataLoader\n",
    "from models.stmfnet import STMFNet\n",
    "from models.student import student_STMFNet\n",
    "import models\n",
    "import losses\n",
    "import datetime\n",
    "from os.path import join\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class testArgs:\n",
    "#     gpu_id = 0\n",
    "#     net = 'STMFNet'\n",
    "#     dataset = 'Ucf101_quintuplet'\n",
    "#     metrics = ['PSNR', 'SSIM']\n",
    "#     checkpoint = './train_results/checkpoint/model_epoch008.pth'\n",
    "#     # checkpoint = './models/stmfnet.pth'\n",
    "#     data_dir = 'D:/stmfnet_data'\n",
    "#     out_dir = './tests/results'\n",
    "#     featc = [64, 128, 256, 512]\n",
    "#     featnet = 'UMultiScaleResNext'\n",
    "#     featnorm = 'batch'\n",
    "#     kernel_size = 5\n",
    "#     dilation = 1\n",
    "#     finetune_pwc = False\n",
    "\n",
    "# class trainArgs:\n",
    "#     gpu_id = 0\n",
    "#     net = 'STMFNet'\n",
    "#     data_dir = 'D:/stmfnet_data'\n",
    "#     out_dir = './train_results'\n",
    "#     load = None\n",
    "#     epochs = 70\n",
    "#     batch_size = 2\n",
    "#     loss = \"1*Lap\"\n",
    "#     patch_size = 256\n",
    "#     lr = 0.001\n",
    "#     lr_decay = 20\n",
    "#     decay_type = 'step'\n",
    "#     gamma = 0.5\n",
    "#     patience = None\n",
    "#     optimizer = 'ADAMax'\n",
    "#     weight_decay = 0\n",
    "#     featc = [32, 64, 96, 128]\n",
    "#     featnet = 'UMultiScaleResNext'\n",
    "#     featnorm = 'batch'\n",
    "#     kernel_size = 5\n",
    "#     dilation = 1\n",
    "#     finetune_pwc = False\n",
    "#     teacher = \"STMFNet\"\n",
    "#     student = \"student_STMFNet\"\n",
    "#     temp = 10\n",
    "#     alpha = 0.1\n",
    "#     distill_loss_fn = \"KLDivLoss\"\n",
    "\n",
    "# args=trainArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Comparison Automation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class trainArgs:\n",
    "#     gpu_id = 0\n",
    "#     net = 'STMFNet'\n",
    "#     data_dir = 'D:/stmfnet_data'\n",
    "#     out_dir = './train_results'\n",
    "#     load = None\n",
    "#     epochs = 70\n",
    "#     batch_size = 2\n",
    "#     loss = \"1*Lap\"\n",
    "#     patch_size = 256\n",
    "#     lr = 0.001\n",
    "#     lr_decay = 20\n",
    "#     decay_type = 'step'\n",
    "#     gamma = 0.5\n",
    "#     patience = None\n",
    "#     optimizer = 'ADAMax'\n",
    "#     weight_decay = 0\n",
    "#     featc = [32, 64, 96, 128]\n",
    "#     featnet = 'UMultiScaleResNext'\n",
    "#     featnorm = 'batch'\n",
    "#     kernel_size = 5\n",
    "#     dilation = 1\n",
    "#     finetune_pwc = False\n",
    "#     temp = 10\n",
    "#     alpha = 0.3\n",
    "#     distill_loss_fn = 'KLDivLoss'\n",
    "\n",
    "# args=trainArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_device(data, device):\n",
    "#     if isinstance(data, (list,tuple)):\n",
    "#         return [to_device(x, device) for x in data]\n",
    "#     return data.to(device, non_blocking=True)\n",
    "\n",
    "# teacher = to_device(STMFNet(args), device)\n",
    "# teacher.to(device)\n",
    "# t_checkpoint = torch.load('./models/stmfnet.pth')\n",
    "# teacher.load_state_dict(t_checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "# model = to_device(STMFNet(args), device)\n",
    "# model.to(device)\n",
    "# m_checkpoint = torch.load(\"./train_results/checkpoint/model_epoch016.pth\")\n",
    "# model.load_state_dict(m_checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student = to_device(student_STMFNet(args), device)\n",
    "# student.to(device)\n",
    "# m_checkpoint = torch.load(\"./distill_results/checkpoint/model_epoch001.pth\")\n",
    "# student.load_state_dict(m_checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_summary = summary(student, [(2, 3, 256, 256), (2, 3, 256, 256), (2, 3, 256, 256), (2, 3, 256, 256)])\n",
    "# with open('./summaries/student.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(str(student_summary))\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate number of zeroes in each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_weights = [w for name,\n",
    "#                    w in teacher.named_parameters() if \"weight\" in name]\n",
    "# model_weights = [w for name, w in model.named_parameters() if \"weight\" in name]\n",
    "# layer_names = [name for name, w in model.named_parameters()\n",
    "#                if \"weight\" in name and len(w.shape) > 1]\n",
    "# K_parameters = np.array([np.prod(w.shape)\n",
    "#                         for w in model_weights if len(w.shape) > 1])\n",
    "# layer_shapes = [w.shape for w in model_weights if len(w.shape) > 1]\n",
    "# num_zeros = np.array([np.count_nonzero(w.detach().cpu() == 0)\n",
    "#                      for w in model_weights if len(w.shape) > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find compression ratios for each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compression_ratios = 1 - (num_zeros / K_parameters)\n",
    "\n",
    "\n",
    "# layer_shapes_df = pd.DataFrame(layer_shapes, columns=['C_out','C_in','q1','q2','q3']).fillna(0)\n",
    "# # df1 = pd.DataFrame(layer_shapes, columns=['layer_shapes'])\n",
    "# df = pd.DataFrame(num_zeros, columns=['num_zeros'])\n",
    "# df['compression_ratios'] = compression_ratios\n",
    "# df['layer_shapes'] = layer_shapes\n",
    "# # df['parameters'] = K_parameters\n",
    "# data = pd.concat([layer_shapes_df, df], axis=1)\n",
    "\n",
    "\n",
    "# data['new_C_in'] = round(data['C_in'] * data['compression_ratios'])\n",
    "# data['difference'] = (abs(data['C_in'] - data['new_C_in'])) #  / ((data['C_in'] + data['new_C_in'])/2)\n",
    "\n",
    "# print(max(data['difference']))\n",
    "# # data = data[data['difference'] > 0.3]\n",
    "# data['layer_names'] = layer_names\n",
    "# data.to_csv('./layer_shapes.csv')\n",
    "\n",
    "# data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_summary = summary(teacher, [(2, 3, 256, 256), (2, 3, 256, 256), (2, 3, 256, 256), (2, 3, 256, 256)])\n",
    "# with open('./summaries/teacher.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(str(teacher_summary))\n",
    "#     f.close()\n",
    "\n",
    "# model_summary = summary(model, [(2, 3, 256, 256), (2, 3, 256, 256), (2, 3, 256, 256), (2, 3, 256, 256)])\n",
    "# with open('./summaries/model.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(str(model_summary))\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Distillation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testArgs:\n",
    "    gpu_id = 0\n",
    "    net = \"STMFNet\"\n",
    "    dataset = \"Snufilm_extreme_quintuplet\"\n",
    "    metrics = [\"PSNR\", \"SSIM\"]\n",
    "    # checkpoint = './models/stmfnet.pth'\n",
    "    data_dir = \"D:/stmfnet_data\"\n",
    "    out_dir = \"./tests/results\"\n",
    "    featc = [32, 64, 96, 128]\n",
    "    featnet = \"UMultiScaleResNext\"\n",
    "    featnorm = \"batch\"\n",
    "    kernel_size = 5\n",
    "    dilation = 1\n",
    "    finetune_pwc = False\n",
    "\n",
    "\n",
    "class trainArgs:\n",
    "    gpu_id = 0\n",
    "    net = \"STMFNet\"\n",
    "    data_dir = \"D:/stmfnet_data\"\n",
    "    out_dir = \"./train_results\"\n",
    "    load = None\n",
    "    epochs = 70\n",
    "    batch_size = 2\n",
    "    loss = \"1*Lap\"\n",
    "    patch_size = 256\n",
    "    lr = 0.001\n",
    "    lr_decay = 20\n",
    "    decay_type = \"step\"\n",
    "    gamma = 0.5\n",
    "    patience = None\n",
    "    optimizer = \"ADAMax\"\n",
    "    weight_decay = 0\n",
    "    featc = [32, 64, 96, 128]\n",
    "    featnet = \"UMultiScaleResNext\"\n",
    "    featnorm = \"batch\"\n",
    "    kernel_size = 5\n",
    "    dilation = 1\n",
    "    finetune_pwc = False\n",
    "    teacher = \"STMFNet\"\n",
    "    student = \"student_STMFNet\"\n",
    "    temp = 10\n",
    "    alpha = 0.1\n",
    "    distill_loss_fn = \"KLDivLoss\"\n",
    "\n",
    "    metrics = [\"PSNR\", \"SSIM\"]\n",
    "\n",
    "\n",
    "args = trainArgs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(args.gpu_id)\n",
    "\n",
    "# training sets\n",
    "vimeo90k_train = Vimeo90k_quintuplet(\n",
    "    join(args.data_dir, \"vimeo_septuplet\"),\n",
    "    train=True,\n",
    "    crop_sz=(args.patch_size, args.patch_size),\n",
    ")\n",
    "bvidvc_train = BVIDVC_quintuplet(\n",
    "    join(args.data_dir, \"bvidvc\"), crop_sz=(args.patch_size, args.patch_size)\n",
    ")\n",
    "\n",
    "# validation set\n",
    "vimeo90k_valid = Vimeo90k_quintuplet(\n",
    "    join(args.data_dir, \"vimeo_septuplet\"),\n",
    "    train=False,\n",
    "    crop_sz=(args.patch_size, args.patch_size),\n",
    "    augment_s=False,\n",
    "    augment_t=False,\n",
    ")\n",
    "\n",
    "datasets_train = [bvidvc_train]\n",
    "train_sampler = Sampler(datasets_train, iter=True)\n",
    "\n",
    "# data loaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_sampler, batch_size=args.batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset=vimeo90k_valid, batch_size=args.batch_size, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "torch.cuda.set_device(args.gpu_id)\n",
    "\n",
    "if not os.path.exists(args.out_dir):\n",
    "    os.mkdir(args.out_dir)\n",
    "\n",
    "\n",
    "teacher = getattr(models, args.teacher)(args).cuda()\n",
    "def load_model(filepath):\n",
    "\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = STMFNet(args).cuda()\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_model(\"./models/stmfnet.pth\")\n",
    "\n",
    "teacher = STMFNet(args).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = getattr(models, args.student)(args).cuda()\n",
    "student.to(device);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distillation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args=trainArgs()\n",
    "\n",
    "# softmax_optimiser = nn.Softmax(dim=1)\n",
    "# mse_loss_function = nn.MSELoss()\n",
    "\n",
    "# def my_loss(scores, targets, temperature = 5):\n",
    "#     soft_pred = softmax_optimiser(scores / temperature)\n",
    "#     soft_targets = softmax_optimiser(targets / temperature)\n",
    "#     loss = mse_loss_function(soft_pred, soft_targets)\n",
    "#     return loss\n",
    "\n",
    "# distil_optimizer = optim.Adam(student.parameters(), lr=0.0001)\n",
    "\n",
    "# losses = []\n",
    "\n",
    "# for epoch in range(5):\n",
    "\n",
    "# \trunning_loss = 0.0\n",
    "# \tfor i, data in enumerate(train_loader, 1):\n",
    "\n",
    "# \t\tinputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "# \t\ttargets = teacher(inputs)\n",
    "# \t\tscores = student(inputs)\n",
    "# \t\tloss = my_loss(scores, targets, temperature = 2)\n",
    "# \t\tdistil_optimizer.zero_grad()\n",
    "# \t\tloss.backward()\n",
    "# \t\tdistil_optimizer.step()\n",
    "\n",
    "# \t\t# print statistics\n",
    "# \t\trunning_loss += loss.item()\n",
    "# \t\tif i % 60 == 59:    # print every 60 mini-batches\n",
    "# \t\t\tprint(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 60:.3f}')\n",
    "# \t\t\trunning_loss = 0.0\n",
    "\t\t\n",
    "# \tprint('appending loss: ', loss.item())\n",
    "# \tlosses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "\n",
    "# student.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = trainArgs()\n",
    "# args.loss = \"1*Lap\"\n",
    "\n",
    "# import losses\n",
    "# loss = losses.DistillationLoss(args)\n",
    "\n",
    "# start_epoch = 0\n",
    "# # if args.load is not None:\n",
    "# #     checkpoint = torch.load(args.load)\n",
    "# #     student.load_state_dict(checkpoint[\"state_dict\"])\n",
    "# #     start_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "# distill_optimizer = optim.Adam(student.parameters(), lr=0.0001)\n",
    "# my_trainer = Distiller(args, train_loader, valid_loader, student, teacher, loss, start_epoch)\n",
    "\n",
    "# # now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "# # with open(join(args.out_dir, \"config.txt\"), \"a\") as f:\n",
    "# #     f.write(now + \"\\n\\n\")\n",
    "# #     for arg in vars(args):\n",
    "# #         f.write(\"{}: {}\\n\".format(arg, getattr(args, arg)))\n",
    "# #     f.write(\"\\n\")\n",
    "\n",
    "# # while not my_trainer.terminate():\n",
    "# #     my_trainer.train()\n",
    "# #     my_trainer.save_checkpoint()\n",
    "# #     my_trainer.validate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=testArgs()\n",
    "args.dataset = 'Ucf101_quintuplet'\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "def getmax():\n",
    "    nums = []\n",
    "    for file in os.listdir('./train_results/checkpoint'):\n",
    "        if file.endswith('.pth'):\n",
    "            nums.append(int(file.split('_epoch')[-1].split('.')[0]))\n",
    "    return max(nums)\n",
    "\n",
    "# for i in range(getmax()):\n",
    "model = to_device(STMFNet(args), device)\n",
    "model.to(device)\n",
    "model_name = \"model_epoch\" + str(31).zfill(3)\n",
    "model_path = \"./train_results/checkpoint/\" + model_name + \".pth\"\n",
    "\n",
    "model_path = \"./models/stmfnet.pth\"\n",
    "\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "print(\"Testing \", model_name, \" on dataset: \", args.dataset)\n",
    "test_dir = os.path.join(args.out_dir, args.dataset)\n",
    "if args.dataset.split(\"_\")[0] in [\"VFITex\", \"Ucf101\", \"Davis90\"]:\n",
    "    db_folder = args.dataset.split(\"_\")[0].lower()\n",
    "else:\n",
    "    db_folder = args.dataset.lower()\n",
    "test_db = getattr(testsets, args.dataset)(os.path.join(args.data_dir, db_folder))\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "\n",
    "test_db.eval(model, metrics=args.metrics, output_dir=test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to show time by process_time() \n",
    "from time import process_time\n",
    "  \n",
    "# assigning n = 50 \n",
    "n = 50\n",
    "  \n",
    "# Start the stopwatch / counter \n",
    "t1_start = process_time() \n",
    "   \n",
    "for i in range(n):\n",
    "    print(i, end =' ')\n",
    "  \n",
    "print() \n",
    "  \n",
    "# Stop the stopwatch / counter\n",
    "t1_stop = process_time()\n",
    "   \n",
    "print(\"Elapsed time:\", t1_stop, t1_start) \n",
    "   \n",
    "print(\"Elapsed time during the whole program in seconds:\",\n",
    "                                         t1_stop-t1_start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=[\"PSNR\", \"SSIM\"]\n",
    "results_dict = {k: [] for k in metrics}\n",
    "results_dict[\"Times\"] = []\n",
    "results_dict[\"Frames\"] = []\n",
    "\n",
    "results_dict['Times'].append(16)\n",
    "results_dict['Times'].append(14)\n",
    "\n",
    "results_dict['Frames'].append(38)\n",
    "results_dict['Frames'].append(32)\n",
    "\n",
    "\n",
    "\n",
    "np.sum(results_dict[\"Times\"]) / np.sum(results_dict[\"Frames\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# from os.path import join, exists\n",
    "# import utility\n",
    "# from torchvision.utils import save_image as imwrite\n",
    "\n",
    "# db_dir = 'D:/stmfnet_data/ucf101'\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# im_list = os.listdir(db_dir)\n",
    "\n",
    "# input1_list = []\n",
    "# input3_list = []\n",
    "# input5_list = []\n",
    "# input7_list = []\n",
    "# gt_list = []\n",
    "# for item in im_list:\n",
    "#     input1_list.append(\n",
    "#         transform(Image.open(join(db_dir, item, \"frame0.png\")))\n",
    "#         .cuda()\n",
    "#         .unsqueeze(0)\n",
    "#     )\n",
    "#     input3_list.append(\n",
    "#         transform(Image.open(join(db_dir, item, \"frame1.png\")))\n",
    "#         .cuda()\n",
    "#         .unsqueeze(0)\n",
    "#     )\n",
    "#     input5_list.append(\n",
    "#         transform(Image.open(join(db_dir, item, \"frame2.png\")))\n",
    "#         .cuda()\n",
    "#         .unsqueeze(0)\n",
    "#     )\n",
    "#     input7_list.append(\n",
    "#         transform(Image.open(join(db_dir, item, \"frame3.png\")))\n",
    "#         .cuda()\n",
    "#         .unsqueeze(0)\n",
    "#     )\n",
    "#     gt_list.append(\n",
    "#         transform(Image.open(join(db_dir, item, \"framet.png\")))\n",
    "#         .cuda()\n",
    "#         .unsqueeze(0)\n",
    "#     )\n",
    "\n",
    "# # def eval(model, , output_dir=None, output_name=\"output.png\"):\n",
    "# # model.eval()\n",
    "\n",
    "# output_dir = \"./tests/\"\n",
    "# output_name = \"output.png\"\n",
    "\n",
    "\n",
    "# # results_dict = {k: [] for k in metrics}\n",
    "\n",
    "# # logfile = open(join(output_dir, \"results.txt\"), \"a\")\n",
    "\n",
    "# for idx in range(len(im_list)):\n",
    "#     if not exists(join(output_dir, im_list[idx])):\n",
    "#         os.makedirs(join(output_dir, im_list[idx]))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         out = teacher(\n",
    "#             input1_list[idx],\n",
    "#             input3_list[idx],\n",
    "#             input5_list[idx],\n",
    "#             input7_list[idx],\n",
    "#         )\n",
    "#     gt = gt_list[idx]\n",
    "\n",
    "\n",
    "#     imwrite(out, join(output_dir, im_list[idx], output_name), range=(0, 1))\n",
    "\n",
    "# #     msg = (\n",
    "# #         \"{:<15s} -- {}\".format(\n",
    "# #             im_list[idx],\n",
    "# #             {k: round(results_dict[k][-1], 3) for k in metrics},\n",
    "# #         )\n",
    "# #         + \"\\n\"\n",
    "# #     )\n",
    "# #     print(msg, end=\"\")\n",
    "# #     logfile.write(msg)\n",
    "\n",
    "# # msg = (\n",
    "# #     \"{:<15s} -- {}\".format(\n",
    "# #         \"Average\", {k: round(np.mean(results_dict[k]), 3) for k in metrics}\n",
    "# #     )\n",
    "# #     + \"\\n\\n\"\n",
    "# # )\n",
    "# # print(msg, end=\"\")\n",
    "# # logfile.write(msg)\n",
    "# # logfile.close()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Shape Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmax():\n",
    "    nums = []\n",
    "    for file in os.listdir('./train_results/checkpoint'):\n",
    "        if file.endswith('.pth'):\n",
    "            nums.append(int(file.split('_epoch')[-1].split('.')[0]))\n",
    "    return max(nums)\n",
    "\n",
    "df = pd.DataFrame(compression_ratios, columns=['compression_ratios'])\n",
    "for i in range(getmax()):\n",
    "    model = to_device(STMFNet(args), device)\n",
    "    model.to(device)\n",
    "    model_name = \"model_epoch\" + str(i+1).zfill(3)\n",
    "    model_path = \"./train_results/checkpoint/\" + model_name + \".pth\"\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    teacher_weights = [w for name,\n",
    "                    w in teacher.named_parameters() if \"weight\" in name]\n",
    "    model_weights = [w for name, w in model.named_parameters() if \"weight\" in name]\n",
    "    layer_names = [name for name, w in model.named_parameters()\n",
    "                if \"weight\" in name and len(w.shape) > 1]\n",
    "    K_parameters = np.array([np.prod(w.shape)\n",
    "                            for w in model_weights if len(w.shape) > 1])\n",
    "    layer_shapes = [w.shape for w in model_weights if len(w.shape) > 1]\n",
    "    num_zeros = np.array([np.count_nonzero(w.detach().cpu() == 0)\n",
    "                        for w in model_weights if len(w.shape) > 1])\n",
    "    compression_ratios = 1 - (num_zeros / K_parameters)\n",
    " \n",
    "    weights = [w for name, w in model.named_parameters() if \"weight\" in name]\n",
    "    num_features = sum([w.numel() for w in weights])\n",
    "    density = sum([torch.sum(w != 0).item() for w in weights]) / num_features\n",
    "    df[str(density)] = compression_ratios\n",
    "\n",
    "df.to_csv('./predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('stmfnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5be8633809c4d4a6fd98bbeceba3cc69f691646712380d18efdcb68d5866f12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
