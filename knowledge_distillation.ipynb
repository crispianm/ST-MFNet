{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKcOCevplC00"
      },
      "source": [
        "# Knowledge Distillation\n",
        "\n",
        "**Author:** [Kenneth Borup](https://twitter.com/Kennethborup)<br>\n",
        "**Date created:** 2020/09/01<br>\n",
        "**Last modified:** 2020/09/01<br>\n",
        "**Description:** Implementation of classical Knowledge Distillation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_O_o-vjlC02"
      },
      "source": [
        "## Introduction to Knowledge Distillation\n",
        "\n",
        "Knowledge Distillation is a procedure for model\n",
        "compression, in which a small (student) model is trained to match a large pre-trained\n",
        "(teacher) model. Knowledge is transferred from the teacher model to the student\n",
        "by minimizing a loss function, aimed at matching softened teacher logits as well as\n",
        "ground-truth labels.\n",
        "\n",
        "The logits are softened by applying a \"temperature\" scaling function in the softmax,\n",
        "effectively smoothing out the probability distribution and revealing\n",
        "inter-class relationships learned by the teacher.\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "- [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9i2UMZOlC03"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vm0nRj6vlC03"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6292\\3996157448.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CxXi7IAXlC04"
      },
      "outputs": [],
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=3,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTqrpOhKlC05"
      },
      "source": [
        "## Create student and teacher models\n",
        "\n",
        "Initialy, we create a teacher model and a smaller student model. Both models are\n",
        "convolutional neural networks and created using `Sequential()`,\n",
        "but could be any Keras model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KLk1WWEnlC06"
      },
      "outputs": [],
      "source": [
        "# Create the teacher\n",
        "teacher = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"teacher\",\n",
        ")\n",
        "\n",
        "# Create the student\n",
        "student = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"student\",\n",
        ")\n",
        "\n",
        "# Clone student for later comparison\n",
        "student_scratch = keras.models.clone_model(student)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.sequential.Sequential at 0x24b05380970>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN4TUSFvlC06"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "The dataset used for training the teacher and distilling the teacher is\n",
        "[MNIST](https://keras.io/api/datasets/mnist/), and the procedure would be equivalent for any other\n",
        "dataset, e.g. [CIFAR-10](https://keras.io/api/datasets/cifar10/), with a suitable choice\n",
        "of models. Both the student and teacher are trained on the training set and evaluated on\n",
        "the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1Xdfx-NLlC07"
      },
      "outputs": [],
      "source": [
        "# Prepare the train and test dataset.\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  []\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR9_kbowlC07"
      },
      "source": [
        "## Train the teacher\n",
        "\n",
        "In knowledge distillation we assume that the teacher is trained and fixed. Thus, we start\n",
        "by training the teacher model on the training set in the usual way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CP5ncPkzlC07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "  77/1875 [>.............................] - ETA: 2:05 - loss: 0.6481 - sparse_categorical_accuracy: 0.7926"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\wg19671\\Desktop\\ST-MFNet\\knowledge_distillation.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wg19671/Desktop/ST-MFNet/knowledge_distillation.ipynb#ch0000011?line=1'>2</a>\u001b[0m teacher\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wg19671/Desktop/ST-MFNet/knowledge_distillation.ipynb#ch0000011?line=2'>3</a>\u001b[0m     optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wg19671/Desktop/ST-MFNet/knowledge_distillation.ipynb#ch0000011?line=3'>4</a>\u001b[0m     loss\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wg19671/Desktop/ST-MFNet/knowledge_distillation.ipynb#ch0000011?line=4'>5</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[keras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mSparseCategoricalAccuracy()],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wg19671/Desktop/ST-MFNet/knowledge_distillation.ipynb#ch0000011?line=5'>6</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wg19671/Desktop/ST-MFNet/knowledge_distillation.ipynb#ch0000011?line=7'>8</a>\u001b[0m \u001b[39m# Train and evaluate teacher on data.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wg19671/Desktop/ST-MFNet/knowledge_distillation.ipynb#ch0000011?line=8'>9</a>\u001b[0m teacher\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wg19671/Desktop/ST-MFNet/knowledge_distillation.ipynb#ch0000011?line=9'>10</a>\u001b[0m teacher\u001b[39m.\u001b[39mevaluate(x_test, y_test)\n",
            "File \u001b[1;32mc:\\Users\\wg19671\\Anaconda3\\envs\\stmfnet\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:108\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_method_wrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    107\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_multi_worker_mode():  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    110\u001b[0m   \u001b[39m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m   \u001b[39mif\u001b[39;00m dc_context\u001b[39m.\u001b[39mget_current_worker_context():\n",
            "File \u001b[1;32mc:\\Users\\wg19671\\Anaconda3\\envs\\stmfnet\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1098\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1092\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mTraceContext\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1093\u001b[0m     graph_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1094\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1095\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1096\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size):\n\u001b[0;32m   1097\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1098\u001b[0m   tmp_logs \u001b[39m=\u001b[39m train_function(iterator)\n\u001b[0;32m   1099\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1100\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\wg19671\\Anaconda3\\envs\\stmfnet\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:780\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m   compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 780\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    782\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tracing_count()\n\u001b[0;32m    783\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\wg19671\\Anaconda3\\envs\\stmfnet\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:807\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    804\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    805\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    806\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    809\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    810\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    811\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\wg19671\\Anaconda3\\envs\\stmfnet\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2829\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2828\u001b[0m   graph_function, args, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2829\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_filtered_call(args, kwargs)\n",
            "File \u001b[1;32mc:\\Users\\wg19671\\Anaconda3\\envs\\stmfnet\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1843\u001b[0m, in \u001b[0;36mConcreteFunction._filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_filtered_call\u001b[39m(\u001b[39mself\u001b[39m, args, kwargs, cancellation_manager\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1828\u001b[0m   \u001b[39m\"\"\"Executes the function, filtering arguments from the Python function.\u001b[39;00m\n\u001b[0;32m   1829\u001b[0m \n\u001b[0;32m   1830\u001b[0m \u001b[39m  Objects aside from Tensors, CompositeTensors, and Variables are ignored.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[39m    `args` and `kwargs`.\u001b[39;00m\n\u001b[0;32m   1842\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1843\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   1844\u001b[0m       [t \u001b[39mfor\u001b[39;49;00m t \u001b[39min\u001b[39;49;00m nest\u001b[39m.\u001b[39;49mflatten((args, kwargs), expand_composites\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1845\u001b[0m        \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(t, (ops\u001b[39m.\u001b[39;49mTensor,\n\u001b[0;32m   1846\u001b[0m                          resource_variable_ops\u001b[39m.\u001b[39;49mBaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m       captured_inputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcaptured_inputs,\n\u001b[0;32m   1848\u001b[0m       cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\wg19671\\Anaconda3\\envs\\stmfnet\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1923\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1918\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m (\n\u001b[0;32m   1919\u001b[0m     pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_TapeSetPossibleGradientTypes(args))\n\u001b[0;32m   1920\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m _POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1921\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1922\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1923\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1924\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1925\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m     args,\n\u001b[0;32m   1927\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1928\u001b[0m     executing_eagerly)\n\u001b[0;32m   1929\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\wg19671\\Anaconda3\\envs\\stmfnet\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:545\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    544\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 545\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    546\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    547\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    548\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    549\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    550\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    551\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    553\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    554\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    558\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\wg19671\\Anaconda3\\envs\\stmfnet\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train teacher as usual\n",
        "teacher.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate teacher on data.\n",
        "teacher.fit(x_train, y_train, epochs=5)\n",
        "teacher.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O8qlVsGlC07"
      },
      "source": [
        "## Distill teacher to student\n",
        "\n",
        "We have already trained the teacher model, and we only need to initialize a\n",
        "`Distiller(student, teacher)` instance, `compile()` it with the desired losses,\n",
        "hyperparameters and optimizer, and distill the teacher to the student."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy9ClWEnlC08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 22s 12ms/step - sparse_categorical_accuracy: 0.9314 - student_loss: 0.2305 - distillation_loss: 0.0318\n",
            "313/313 [==============================] - 0s 1ms/step - sparse_categorical_accuracy: 0.9724 - student_loss: 0.0841\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9724000096321106"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize and compile distiller\n",
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.1,\n",
        "    temperature=10,\n",
        ")\n",
        "\n",
        "# Distill teacher to student\n",
        "distiller.fit(x_train, y_train, epochs=3)\n",
        "\n",
        "# Evaluate student on test dataset\n",
        "distiller.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLI2BtGblC08"
      },
      "source": [
        "## Train student from scratch for comparison\n",
        "\n",
        "We can also train an equivalent student model from scratch without the teacher, in order\n",
        "to evaluate the performance gain obtained by knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1_e9xinlC08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2355 - sparse_categorical_accuracy: 0.9307\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0916 - sparse_categorical_accuracy: 0.9721\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.09162777662277222, 0.972100019454956]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train student as doen usually\n",
        "student_scratch.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate student trained from scratch.\n",
        "student_scratch.fit(x_train, y_train, epochs=3)\n",
        "student_scratch.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZGBHi4_lC08"
      },
      "source": [
        "If the teacher is trained for 5 full epochs and the student is distilled on this teacher\n",
        "for 3 full epochs, you should in this example experience a performance boost compared to\n",
        "training the same student model from scratch, and even compared to the teacher itself.\n",
        "You should expect the teacher to have accuracy around 97.6%, the student trained from\n",
        "scratch should be around 97.6%, and the distilled student should be around 98.1%. Remove\n",
        "or try out different seeds to use different weight initializations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Messing about\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "knowledge_distillation",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('stmfnet')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d5be8633809c4d4a6fd98bbeceba3cc69f691646712380d18efdcb68d5866f12"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
