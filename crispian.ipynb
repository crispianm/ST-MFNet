{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from data import testsets\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from data.datasets import *\n",
    "from trainers.distiller import Distiller\n",
    "from torch.utils.data import DataLoader\n",
    "from models.stmfnet import STMFNet\n",
    "from models.student import student_STMFNet\n",
    "import models\n",
    "import losses\n",
    "import datetime\n",
    "from os.path import join\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class testArgs:\n",
    "#     gpu_id = 0\n",
    "#     net = 'STMFNet'\n",
    "#     dataset = 'Ucf101_quintuplet'\n",
    "#     metrics = ['PSNR', 'SSIM']\n",
    "#     checkpoint = './train_results/checkpoint/model_epoch008.pth'\n",
    "#     # checkpoint = './models/stmfnet.pth'\n",
    "#     data_dir = 'D:/stmfnet_data'\n",
    "#     out_dir = './tests/results'\n",
    "#     featc = [64, 128, 256, 512]\n",
    "#     featnet = 'UMultiScaleResNext'\n",
    "#     featnorm = 'batch'\n",
    "#     kernel_size = 5\n",
    "#     dilation = 1\n",
    "#     finetune_pwc = False\n",
    "\n",
    "# class trainArgs:\n",
    "#     gpu_id = 0\n",
    "#     net = 'STMFNet'\n",
    "#     data_dir = 'D:/stmfnet_data'\n",
    "#     out_dir = './train_results'\n",
    "#     load = None\n",
    "#     epochs = 70\n",
    "#     batch_size = 2\n",
    "#     loss = \"1*Lap\"\n",
    "#     patch_size = 256\n",
    "#     lr = 0.001\n",
    "#     lr_decay = 20\n",
    "#     decay_type = 'step'\n",
    "#     gamma = 0.5\n",
    "#     patience = None\n",
    "#     optimizer = 'ADAMax'\n",
    "#     weight_decay = 0\n",
    "#     featc = [32, 64, 96, 128]\n",
    "#     featnet = 'UMultiScaleResNext'\n",
    "#     featnorm = 'batch'\n",
    "#     kernel_size = 5\n",
    "#     dilation = 1\n",
    "#     finetune_pwc = False\n",
    "#     teacher = \"STMFNet\"\n",
    "#     student = \"student_STMFNet\"\n",
    "#     temp = 10\n",
    "#     alpha = 0.1\n",
    "#     distill_loss_fn = \"KLDivLoss\"\n",
    "\n",
    "# args=trainArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Comparison Automation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class trainArgs:\n",
    "#     gpu_id = 0\n",
    "#     net = 'STMFNet'\n",
    "#     data_dir = 'D:/stmfnet_data'\n",
    "#     out_dir = './train_results'\n",
    "#     load = None\n",
    "#     epochs = 70\n",
    "#     batch_size = 2\n",
    "#     loss = \"1*Lap\"\n",
    "#     patch_size = 256\n",
    "#     lr = 0.001\n",
    "#     lr_decay = 20\n",
    "#     decay_type = 'step'\n",
    "#     gamma = 0.5\n",
    "#     patience = None\n",
    "#     optimizer = 'ADAMax'\n",
    "#     weight_decay = 0\n",
    "#     featc = [32, 64, 96, 128]\n",
    "#     featnet = 'UMultiScaleResNext'\n",
    "#     featnorm = 'batch'\n",
    "#     kernel_size = 5\n",
    "#     dilation = 1\n",
    "#     finetune_pwc = False\n",
    "#     temp = 10\n",
    "#     alpha = 0.3\n",
    "#     distill_loss_fn = 'KLDivLoss'\n",
    "\n",
    "# args=trainArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_device(data, device):\n",
    "#     if isinstance(data, (list,tuple)):\n",
    "#         return [to_device(x, device) for x in data]\n",
    "#     return data.to(device, non_blocking=True)\n",
    "\n",
    "# teacher = to_device(STMFNet(args), device)\n",
    "# teacher.to(device)\n",
    "# t_checkpoint = torch.load('./models/stmfnet.pth')\n",
    "# teacher.load_state_dict(t_checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "# model = to_device(STMFNet(args), device)\n",
    "# model.to(device)\n",
    "# m_checkpoint = torch.load(\"./train_results/checkpoint/model_epoch016.pth\")\n",
    "# model.load_state_dict(m_checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student = to_device(student_STMFNet(args), device)\n",
    "# student.to(device)\n",
    "# m_checkpoint = torch.load(\"./distill_results/checkpoint/model_epoch001.pth\")\n",
    "# student.load_state_dict(m_checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_summary = summary(student, [(2, 3, 256, 256), (2, 3, 256, 256), (2, 3, 256, 256), (2, 3, 256, 256)])\n",
    "# with open('./summaries/student.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(str(student_summary))\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate number of zeroes in each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_weights = [w for name,\n",
    "#                    w in teacher.named_parameters() if \"weight\" in name]\n",
    "# model_weights = [w for name, w in model.named_parameters() if \"weight\" in name]\n",
    "# layer_names = [name for name, w in model.named_parameters()\n",
    "#                if \"weight\" in name and len(w.shape) > 1]\n",
    "# K_parameters = np.array([np.prod(w.shape)\n",
    "#                         for w in model_weights if len(w.shape) > 1])\n",
    "# layer_shapes = [w.shape for w in model_weights if len(w.shape) > 1]\n",
    "# num_zeros = np.array([np.count_nonzero(w.detach().cpu() == 0)\n",
    "#                      for w in model_weights if len(w.shape) > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find compression ratios for each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compression_ratios = 1 - (num_zeros / K_parameters)\n",
    "\n",
    "\n",
    "# layer_shapes_df = pd.DataFrame(layer_shapes, columns=['C_out','C_in','q1','q2','q3']).fillna(0)\n",
    "# # df1 = pd.DataFrame(layer_shapes, columns=['layer_shapes'])\n",
    "# df = pd.DataFrame(num_zeros, columns=['num_zeros'])\n",
    "# df['compression_ratios'] = compression_ratios\n",
    "# df['layer_shapes'] = layer_shapes\n",
    "# # df['parameters'] = K_parameters\n",
    "# data = pd.concat([layer_shapes_df, df], axis=1)\n",
    "\n",
    "\n",
    "# data['new_C_in'] = round(data['C_in'] * data['compression_ratios'])\n",
    "# data['difference'] = (abs(data['C_in'] - data['new_C_in'])) #  / ((data['C_in'] + data['new_C_in'])/2)\n",
    "\n",
    "# print(max(data['difference']))\n",
    "# # data = data[data['difference'] > 0.3]\n",
    "# data['layer_names'] = layer_names\n",
    "# data.to_csv('./layer_shapes.csv')\n",
    "\n",
    "# data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_summary = summary(teacher, [(2, 3, 256, 256), (2, 3, 256, 256), (2, 3, 256, 256), (2, 3, 256, 256)])\n",
    "# with open('./summaries/teacher.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(str(teacher_summary))\n",
    "#     f.close()\n",
    "\n",
    "# model_summary = summary(model, [(2, 3, 256, 256), (2, 3, 256, 256), (2, 3, 256, 256), (2, 3, 256, 256)])\n",
    "# with open('./summaries/model.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     f.write(str(model_summary))\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Distillation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testArgs:\n",
    "    gpu_id = 0\n",
    "    net = 'STMFNet'\n",
    "    dataset = 'Snufilm_extreme_quintuplet'\n",
    "    metrics = ['PSNR', 'SSIM']\n",
    "    # checkpoint = './models/stmfnet.pth'\n",
    "    data_dir = 'D:/stmfnet_data'\n",
    "    out_dir = './tests/results'\n",
    "    featc = [32, 64, 96, 128]\n",
    "    featnet = 'UMultiScaleResNext'\n",
    "    featnorm = 'batch'\n",
    "    kernel_size = 5\n",
    "    dilation = 1\n",
    "    finetune_pwc = False\n",
    "\n",
    "class trainArgs:\n",
    "    gpu_id = 0\n",
    "    net = 'STMFNet'\n",
    "    data_dir = 'D:/stmfnet_data'\n",
    "    out_dir = './train_results'\n",
    "    load = None\n",
    "    epochs = 70\n",
    "    batch_size = 2\n",
    "    loss = \"1*Lap\"\n",
    "    patch_size = 256\n",
    "    lr = 0.001\n",
    "    lr_decay = 20\n",
    "    decay_type = 'step'\n",
    "    gamma = 0.5\n",
    "    patience = None\n",
    "    optimizer = 'ADAMax'\n",
    "    weight_decay = 0\n",
    "    featc = [32, 64, 96, 128]\n",
    "    featnet = 'UMultiScaleResNext'\n",
    "    featnorm = 'batch'\n",
    "    kernel_size = 5\n",
    "    dilation = 1\n",
    "    finetune_pwc = False\n",
    "    teacher = \"STMFNet\"\n",
    "    student = \"student_STMFNet\"\n",
    "    temp = 10\n",
    "    alpha = 0.1\n",
    "    distill_loss_fn = \"KLDivLoss\"\n",
    "\n",
    "args=trainArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(args.gpu_id)\n",
    "\n",
    "# training sets\n",
    "vimeo90k_train = Vimeo90k_quintuplet(\n",
    "    join(args.data_dir, \"vimeo_septuplet\"),\n",
    "    train=True,\n",
    "    crop_sz=(args.patch_size, args.patch_size),\n",
    ")\n",
    "bvidvc_train = BVIDVC_quintuplet(\n",
    "    join(args.data_dir, \"bvidvc\"), crop_sz=(args.patch_size, args.patch_size)\n",
    ")\n",
    "\n",
    "# validation set\n",
    "vimeo90k_valid = Vimeo90k_quintuplet(\n",
    "    join(args.data_dir, \"vimeo_septuplet\"),\n",
    "    train=False,\n",
    "    crop_sz=(args.patch_size, args.patch_size),\n",
    "    augment_s=False,\n",
    "    augment_t=False,\n",
    ")\n",
    "\n",
    "datasets_train = [bvidvc_train]\n",
    "train_sampler = Sampler(datasets_train, iter=True)\n",
    "\n",
    "# data loaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_sampler, batch_size=args.batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset=vimeo90k_valid, batch_size=args.batch_size, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "torch.cuda.set_device(args.gpu_id)\n",
    "\n",
    "if not os.path.exists(args.out_dir):\n",
    "    os.mkdir(args.out_dir)\n",
    "\n",
    "\n",
    "teacher = getattr(models, args.teacher)(args).cuda()\n",
    "def load_model(filepath):\n",
    "\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = STMFNet(args).cuda()\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_model(\"./models/stmfnet.pth\")\n",
    "\n",
    "teacher = STMFNet(args).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = getattr(models, args.student)(args).cuda()\n",
    "student.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distillation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args=trainArgs()\n",
    "\n",
    "# softmax_optimiser = nn.Softmax(dim=1)\n",
    "# mse_loss_function = nn.MSELoss()\n",
    "\n",
    "# def my_loss(scores, targets, temperature = 5):\n",
    "#     soft_pred = softmax_optimiser(scores / temperature)\n",
    "#     soft_targets = softmax_optimiser(targets / temperature)\n",
    "#     loss = mse_loss_function(soft_pred, soft_targets)\n",
    "#     return loss\n",
    "\n",
    "# distil_optimizer = optim.Adam(student.parameters(), lr=0.0001)\n",
    "\n",
    "# losses = []\n",
    "\n",
    "# for epoch in range(5):\n",
    "\n",
    "# \trunning_loss = 0.0\n",
    "# \tfor i, data in enumerate(train_loader, 1):\n",
    "\n",
    "# \t\tinputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "# \t\ttargets = teacher(inputs)\n",
    "# \t\tscores = student(inputs)\n",
    "# \t\tloss = my_loss(scores, targets, temperature = 2)\n",
    "# \t\tdistil_optimizer.zero_grad()\n",
    "# \t\tloss.backward()\n",
    "# \t\tdistil_optimizer.step()\n",
    "\n",
    "# \t\t# print statistics\n",
    "# \t\trunning_loss += loss.item()\n",
    "# \t\tif i % 60 == 59:    # print every 60 mini-batches\n",
    "# \t\t\tprint(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 60:.3f}')\n",
    "# \t\t\trunning_loss = 0.0\n",
    "\t\t\n",
    "# \tprint('appending loss: ', loss.item())\n",
    "# \tlosses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "\n",
    "# student.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 * Lap\n",
      "\n",
      "Args: {'betas': (0.9, 0.999), 'eps': 1e-08, 'lr': 0.001, 'weight_decay': 0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = trainArgs()\n",
    "args.loss = \"1*Lap\"\n",
    "\n",
    "import losses\n",
    "loss = losses.DistillationLoss(args)\n",
    "\n",
    "start_epoch = 0\n",
    "# if args.load is not None:\n",
    "#     checkpoint = torch.load(args.load)\n",
    "#     student.load_state_dict(checkpoint[\"state_dict\"])\n",
    "#     start_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "distill_optimizer = optim.Adam(student.parameters(), lr=0.0001)\n",
    "my_trainer = Distiller(args, train_loader, valid_loader, student, teacher, loss, start_epoch)\n",
    "\n",
    "# now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "# with open(join(args.out_dir, \"config.txt\"), \"a\") as f:\n",
    "#     f.write(now + \"\\n\\n\")\n",
    "#     for arg in vars(args):\n",
    "#         f.write(\"{}: {}\\n\".format(arg, getattr(args, arg)))\n",
    "#     f.write(\"\\n\")\n",
    "\n",
    "# while not my_trainer.terminate():\n",
    "#     my_trainer.train()\n",
    "#     my_trainer.save_checkpoint()\n",
    "#     my_trainer.validate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing  model_epoch006  on dataset:  Davis90_quintuplet\n",
      "bear            -- {'PSNR': 26.277, 'SSIM': 0.909}\n",
      "bike-packing    -- {'PSNR': 27.58, 'SSIM': 0.91}\n",
      "blackswan       -- {'PSNR': 29.844, 'SSIM': 0.925}\n",
      "bmx-bumps       -- {'PSNR': 21.115, 'SSIM': 0.767}\n",
      "bmx-trees       -- {'PSNR': 21.55, 'SSIM': 0.73}\n",
      "boat            -- {'PSNR': 35.03, 'SSIM': 0.974}\n",
      "boxing-fisheye  -- {'PSNR': 28.173, 'SSIM': 0.953}\n",
      "breakdance      -- {'PSNR': 23.075, 'SSIM': 0.915}\n",
      "breakdance-flare -- {'PSNR': 24.477, 'SSIM': 0.757}\n",
      "bus             -- {'PSNR': 25.311, 'SSIM': 0.861}\n",
      "camel           -- {'PSNR': 29.507, 'SSIM': 0.948}\n",
      "car-roundabout  -- {'PSNR': 23.8, 'SSIM': 0.84}\n",
      "car-shadow      -- {'PSNR': 26.157, 'SSIM': 0.9}\n",
      "car-turn        -- {'PSNR': 28.856, 'SSIM': 0.878}\n",
      "cat-girl        -- {'PSNR': 20.589, 'SSIM': 0.774}\n",
      "classic-car     -- {'PSNR': 16.529, 'SSIM': 0.642}\n",
      "color-run       -- {'PSNR': 22.436, 'SSIM': 0.931}\n",
      "cows            -- {'PSNR': 27.713, 'SSIM': 0.929}\n",
      "crossing        -- {'PSNR': 32.3, 'SSIM': 0.959}\n",
      "dance-jump      -- {'PSNR': 26.201, 'SSIM': 0.826}\n",
      "dance-twirl     -- {'PSNR': 24.028, 'SSIM': 0.779}\n",
      "dancing         -- {'PSNR': 21.641, 'SSIM': 0.834}\n",
      "disc-jockey     -- {'PSNR': 32.527, 'SSIM': 0.976}\n",
      "dog             -- {'PSNR': 24.878, 'SSIM': 0.835}\n",
      "dog-agility     -- {'PSNR': 17.043, 'SSIM': 0.751}\n",
      "dog-gooses      -- {'PSNR': 30.611, 'SSIM': 0.955}\n",
      "dogs-jump       -- {'PSNR': 29.712, 'SSIM': 0.948}\n",
      "dogs-scale      -- {'PSNR': 21.002, 'SSIM': 0.846}\n",
      "drift-chicane   -- {'PSNR': 38.599, 'SSIM': 0.985}\n",
      "drift-straight  -- {'PSNR': 17.213, 'SSIM': 0.521}\n",
      "drift-turn      -- {'PSNR': 20.161, 'SSIM': 0.627}\n",
      "drone           -- {'PSNR': 21.307, 'SSIM': 0.821}\n",
      "elephant        -- {'PSNR': 30.693, 'SSIM': 0.927}\n",
      "flamingo        -- {'PSNR': 30.578, 'SSIM': 0.923}\n",
      "goat            -- {'PSNR': 20.878, 'SSIM': 0.692}\n",
      "gold-fish       -- {'PSNR': 33.974, 'SSIM': 0.994}\n",
      "hike            -- {'PSNR': 25.881, 'SSIM': 0.845}\n",
      "hockey          -- {'PSNR': 30.07, 'SSIM': 0.931}\n",
      "horsejump-high  -- {'PSNR': 24.483, 'SSIM': 0.892}\n",
      "horsejump-low   -- {'PSNR': 21.936, 'SSIM': 0.752}\n",
      "india           -- {'PSNR': 26.084, 'SSIM': 0.906}\n",
      "judo            -- {'PSNR': 34.827, 'SSIM': 0.976}\n",
      "kid-football    -- {'PSNR': 24.765, 'SSIM': 0.908}\n",
      "kite-surf       -- {'PSNR': 28.087, 'SSIM': 0.915}\n",
      "kite-walk       -- {'PSNR': 34.321, 'SSIM': 0.974}\n",
      "koala           -- {'PSNR': 30.625, 'SSIM': 0.953}\n",
      "lab-coat        -- {'PSNR': 25.221, 'SSIM': 0.835}\n",
      "lady-running    -- {'PSNR': 26.795, 'SSIM': 0.931}\n",
      "libby           -- {'PSNR': 22.656, 'SSIM': 0.645}\n",
      "lindy-hop       -- {'PSNR': 25.268, 'SSIM': 0.914}\n",
      "loading         -- {'PSNR': 26.657, 'SSIM': 0.894}\n",
      "longboard       -- {'PSNR': 22.524, 'SSIM': 0.777}\n",
      "lucia           -- {'PSNR': 29.416, 'SSIM': 0.955}\n",
      "mallard-fly     -- {'PSNR': 21.356, 'SSIM': 0.75}\n",
      "mallard-water   -- {'PSNR': 19.845, 'SSIM': 0.657}\n",
      "mbike-trick     -- {'PSNR': 29.847, 'SSIM': 0.879}\n",
      "miami-surf      -- {'PSNR': 29.968, 'SSIM': 0.904}\n",
      "motocross-bumps -- {'PSNR': 22.953, 'SSIM': 0.757}\n",
      "motocross-jump  -- {'PSNR': 18.883, 'SSIM': 0.679}\n",
      "motorbike       -- {'PSNR': 23.659, 'SSIM': 0.729}\n",
      "night-race      -- {'PSNR': 29.213, 'SSIM': 0.862}\n",
      "paragliding     -- {'PSNR': 31.692, 'SSIM': 0.894}\n",
      "paragliding-launch -- {'PSNR': 30.273, 'SSIM': 0.949}\n",
      "parkour         -- {'PSNR': 23.782, 'SSIM': 0.767}\n",
      "pigs            -- {'PSNR': 33.109, 'SSIM': 0.967}\n",
      "planes-water    -- {'PSNR': 44.032, 'SSIM': 0.997}\n",
      "rallye          -- {'PSNR': 31.619, 'SSIM': 0.956}\n",
      "rhino           -- {'PSNR': 30.814, 'SSIM': 0.968}\n",
      "rollerblade     -- {'PSNR': 21.437, 'SSIM': 0.764}\n",
      "schoolgirls     -- {'PSNR': 32.22, 'SSIM': 0.98}\n",
      "scooter-black   -- {'PSNR': 16.625, 'SSIM': 0.62}\n",
      "scooter-board   -- {'PSNR': 18.55, 'SSIM': 0.726}\n",
      "scooter-gray    -- {'PSNR': 19.321, 'SSIM': 0.703}\n",
      "sheep           -- {'PSNR': 35.31, 'SSIM': 0.988}\n",
      "shooting        -- {'PSNR': 17.328, 'SSIM': 0.659}\n",
      "skate-park      -- {'PSNR': 34.549, 'SSIM': 0.971}\n",
      "snowboard       -- {'PSNR': 18.411, 'SSIM': 0.473}\n",
      "soapbox         -- {'PSNR': 26.148, 'SSIM': 0.846}\n",
      "soccerball      -- {'PSNR': 28.622, 'SSIM': 0.952}\n",
      "stroller        -- {'PSNR': 22.674, 'SSIM': 0.75}\n",
      "stunt           -- {'PSNR': 27.055, 'SSIM': 0.911}\n",
      "surf            -- {'PSNR': 27.752, 'SSIM': 0.939}\n",
      "swing           -- {'PSNR': 24.299, 'SSIM': 0.766}\n",
      "tennis          -- {'PSNR': 24.591, 'SSIM': 0.893}\n",
      "tractor-sand    -- {'PSNR': 28.421, 'SSIM': 0.844}\n",
      "train           -- {'PSNR': 26.143, 'SSIM': 0.851}\n",
      "tuk-tuk         -- {'PSNR': 19.239, 'SSIM': 0.689}\n",
      "upside-down     -- {'PSNR': 16.368, 'SSIM': 0.664}\n",
      "varanus-cage    -- {'PSNR': 23.055, 'SSIM': 0.796}\n",
      "walking         -- {'PSNR': 18.742, 'SSIM': 0.692}\n",
      "Average         -- {'PSNR': 25.988, 'SSIM': 0.844}\n"
     ]
    }
   ],
   "source": [
    "args=testArgs()\n",
    "args.dataset = 'Davis90_quintuplet'\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "def getmax():\n",
    "    nums = []\n",
    "    for file in os.listdir('./train_results/checkpoint'):\n",
    "        if file.endswith('.pth'):\n",
    "            nums.append(int(file.split('_epoch')[-1].split('.')[0]))\n",
    "    return max(nums)\n",
    "\n",
    "\n",
    "model = to_device(student_STMFNet(args), device)\n",
    "model.to(device)\n",
    "model_name = \"model_epoch\" + str(getmax()).zfill(3)\n",
    "model_path = \"./train_results/checkpoint/\" + model_name + \".pth\"\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "print(\"Testing \", model_name, \" on dataset: \", args.dataset)\n",
    "test_dir = os.path.join(args.out_dir, args.dataset)\n",
    "if args.dataset.split(\"_\")[0] in [\"VFITex\", \"Ucf101\", \"Davis90\"]:\n",
    "    db_folder = args.dataset.split(\"_\")[0].lower()\n",
    "else:\n",
    "    db_folder = args.dataset.lower()\n",
    "test_db = getattr(testsets, args.dataset)(os.path.join(args.data_dir, db_folder))\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "\n",
    "test_db.eval(model, metrics=args.metrics, output_dir=test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from os.path import join, exists\n",
    "import utility\n",
    "from torchvision.utils import save_image as imwrite\n",
    "\n",
    "db_dir = 'D:/stmfnet_data/ucf101'\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "im_list = os.listdir(db_dir)\n",
    "\n",
    "input1_list = []\n",
    "input3_list = []\n",
    "input5_list = []\n",
    "input7_list = []\n",
    "gt_list = []\n",
    "for item in im_list:\n",
    "    input1_list.append(\n",
    "        transform(Image.open(join(db_dir, item, \"frame0.png\")))\n",
    "        .cuda()\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "    input3_list.append(\n",
    "        transform(Image.open(join(db_dir, item, \"frame1.png\")))\n",
    "        .cuda()\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "    input5_list.append(\n",
    "        transform(Image.open(join(db_dir, item, \"frame2.png\")))\n",
    "        .cuda()\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "    input7_list.append(\n",
    "        transform(Image.open(join(db_dir, item, \"frame3.png\")))\n",
    "        .cuda()\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "    gt_list.append(\n",
    "        transform(Image.open(join(db_dir, item, \"framet.png\")))\n",
    "        .cuda()\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "\n",
    "# def eval(model, , output_dir=None, output_name=\"output.png\"):\n",
    "# model.eval()\n",
    "\n",
    "output_dir = \"./tests/\"\n",
    "output_name = \"output.png\"\n",
    "\n",
    "\n",
    "# results_dict = {k: [] for k in metrics}\n",
    "\n",
    "# logfile = open(join(output_dir, \"results.txt\"), \"a\")\n",
    "\n",
    "for idx in range(len(im_list)):\n",
    "    if not exists(join(output_dir, im_list[idx])):\n",
    "        os.makedirs(join(output_dir, im_list[idx]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = teacher(\n",
    "            input1_list[idx],\n",
    "            input3_list[idx],\n",
    "            input5_list[idx],\n",
    "            input7_list[idx],\n",
    "        )\n",
    "    gt = gt_list[idx]\n",
    "\n",
    "\n",
    "    imwrite(out, join(output_dir, im_list[idx], output_name), range=(0, 1))\n",
    "\n",
    "#     msg = (\n",
    "#         \"{:<15s} -- {}\".format(\n",
    "#             im_list[idx],\n",
    "#             {k: round(results_dict[k][-1], 3) for k in metrics},\n",
    "#         )\n",
    "#         + \"\\n\"\n",
    "#     )\n",
    "#     print(msg, end=\"\")\n",
    "#     logfile.write(msg)\n",
    "\n",
    "# msg = (\n",
    "#     \"{:<15s} -- {}\".format(\n",
    "#         \"Average\", {k: round(np.mean(results_dict[k]), 3) for k in metrics}\n",
    "#     )\n",
    "#     + \"\\n\\n\"\n",
    "# )\n",
    "# print(msg, end=\"\")\n",
    "# logfile.write(msg)\n",
    "# logfile.close()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Shape Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmax():\n",
    "    nums = []\n",
    "    for file in os.listdir('./train_results/checkpoint'):\n",
    "        if file.endswith('.pth'):\n",
    "            nums.append(int(file.split('_epoch')[-1].split('.')[0]))\n",
    "    return max(nums)\n",
    "\n",
    "df = pd.DataFrame(compression_ratios, columns=['compression_ratios'])\n",
    "for i in range(getmax()):\n",
    "    model = to_device(STMFNet(args), device)\n",
    "    model.to(device)\n",
    "    model_name = \"model_epoch\" + str(i+1).zfill(3)\n",
    "    model_path = \"./train_results/checkpoint/\" + model_name + \".pth\"\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    teacher_weights = [w for name,\n",
    "                    w in teacher.named_parameters() if \"weight\" in name]\n",
    "    model_weights = [w for name, w in model.named_parameters() if \"weight\" in name]\n",
    "    layer_names = [name for name, w in model.named_parameters()\n",
    "                if \"weight\" in name and len(w.shape) > 1]\n",
    "    K_parameters = np.array([np.prod(w.shape)\n",
    "                            for w in model_weights if len(w.shape) > 1])\n",
    "    layer_shapes = [w.shape for w in model_weights if len(w.shape) > 1]\n",
    "    num_zeros = np.array([np.count_nonzero(w.detach().cpu() == 0)\n",
    "                        for w in model_weights if len(w.shape) > 1])\n",
    "    compression_ratios = 1 - (num_zeros / K_parameters)\n",
    " \n",
    "    weights = [w for name, w in model.named_parameters() if \"weight\" in name]\n",
    "    num_features = sum([w.numel() for w in weights])\n",
    "    density = sum([torch.sum(w != 0).item() for w in weights]) / num_features\n",
    "    df[str(density)] = compression_ratios\n",
    "\n",
    "df.to_csv('./predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('stmfnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5be8633809c4d4a6fd98bbeceba3cc69f691646712380d18efdcb68d5866f12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
